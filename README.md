# SS21S2026_202010298

**Seminario de Sistemas 2 - Universidad San Carlos de Guatemala**

---

## üìã Tarea 1: Limpieza y An√°lisis Inicial con Pandas

### 1. Descripci√≥n de la Actividad

Esta tarea busca desarrollar habilidades en **limpieza de datos** y **an√°lisis exploratorio** (EDA - Exploratory Data Analysis) utilizando la librer√≠a Pandas de Python. Se trabaja con un dataset inicial "sucio" que contiene diversas problem√°ticas comunes en datos reales, y se procesan para generar un dataset limpio y an√°lisis visuales significativos.

**Dataset:** `dataset_sucio.csv` ‚Üí `dataset_limpio.csv`

---

### 2. Objetivos

- ‚úÖ Eliminar registros duplicados
- ‚úÖ Tratar valores faltantes (NaN) y celdas vac√≠as
- ‚úÖ Estandarizar formatos y valores de texto
- ‚úÖ Limpiar y validar datos num√©ricos
- ‚úÖ Procesar y normalizar fechas
- ‚úÖ Generar tablas pivote para an√°lisis
- ‚úÖ Crear visualizaciones significativas
- ‚úÖ Documentar el proceso y resultados

---

### 3. Descripci√≥n del Proceso

El script `t1.py` implementa un pipeline de limpieza de datos en las siguientes fases:

#### **Fase 1: Carga de Datos**
```python
df = pd.read_csv("dataset_sucio.csv")
```
Se carga el dataset original en un DataFrame de Pandas para procesamiento.

---

#### **Fase 2: Eliminaci√≥n de Duplicados**
Se utiliza `drop_duplicates()` para remover filas completamente id√©nticas:
- **Registros eliminados:** Reducci√≥n de filas duplicadas
- **Impacto:** Garantiza unicidad de registros

---

#### **Fase 3: Estandarizaci√≥n de Columnas de Texto**
Se procesan columnasde tipo texto: `nombre`, `genero`, `ciudad`, `categoria`

**Transformaciones aplicadas:**
1. Llenar valores NaN con "Desconocido"
2. Convertir a min√∫sculas
3. Eliminar espacios en blanco
4. Aplicar Title Case (primera letra may√∫scula)
5. Reemplazar valores raros (`'nan'`, `'none'`, espacios en blanco) por "Desconocido"

---

#### **Fase 4: Estandarizaci√≥n de G√©nero**
Normalizaci√≥n de valores inconsistentes:
- `M`, `m`, `Masculino`, `male` ‚Üí **Masculino**
- `F`, `f`, `Femenino`, `female` ‚Üí **Femenino**
- Otros valores ‚Üí **Desconocido**

---

#### **Fase 5: Limpieza de Datos Num√©ricos (Gasto)**
Procesamiento de la columna `gasto_q`:
1. Conversi√≥n de separadores decimales (`,` ‚Üí `.`)
2. Conversi√≥n a tipo num√©rico con manejo de errores
3. Imputaci√≥n de faltantes con la **mediana** del dataset

---

#### **Fase 6: Estandarizaci√≥n de Fechas**
Conversi√≥n y normalizaci√≥n de formato de fechas:
- **Formato esperado:** dd/mm/yyyy
- **Funci√≥n:** `pd.to_datetime()` con par√°metro `dayfirst=True`

---

#### **Fase 7: An√°lisis y Exportaci√≥n**
- Exportaci√≥n del dataset limpio: `dataset_limpio.csv`
- Generaci√≥n de tablas pivote
- Creaci√≥n de visualizaciones

---

### 4. Resultados y Visualizaciones

#### **4.1 Resumen de Estandarizaci√≥n**

| Columna | Original (√önicos) | Limpio (√önicos) | Reducci√≥n |
|---------|-------------------|-----------------|-----------|
| G√©nero | 15+ variaciones | 3 categor√≠as | ~80% |
| Ciudad | Inconsistencias | Normalizado | Estandarizado |
| Categor√≠a | Variabilidad | 4 categor√≠as | Consistente |

---

#### **4.2 Distribuci√≥n de Gasto**

![Histograma de Distribuci√≥n de Gasto](https://github.com/sebas202010298/SS21S2026_202010298/blob/main/Tarea%201/images/grafica_1_histograma_gasto.png?raw=true)

**Interpretaci√≥n:**
- La distribuci√≥n muestra un comportamiento **multimodal** con una concentraci√≥n significativa alrededor de **Q.280-300**
- Existe una cola derecha que indica algunos valores at√≠picos de gasto alto
- La mayor√≠a de los registros oscilan entre **Q.50 y Q.500**
- **Estad√≠sticas:**
  - M√≠nimo: ~Q.0.50
  - M√°ximo: ~Q.500+
  - Mediana: Utilizada para imputaci√≥n de faltantes

---

#### **4.3 Gasto Total por Ciudad**

![Gasto Total por Ciudad](https://github.com/sebas202010298/SS21S2026_202010298/blob/main/Tarea%201/images/grafica_2_gasto_total_ciudad.png?raw=true)

**Interpretaci√≥n:**
- **Quetzaltenango** lidera con el gasto total m√°s alto (~Q.155,000)
- Ciudades como **Antigua, Mixco, Escuintla, Chimahenango, Guatemala y Antigua** muestran niveles similares (~Q.145,000-150,000)
- **Desconocido** presenta el menor gasto total (~Q.50,000), indicando registros con ubicaci√≥n no identificada
- La distribuci√≥n es **relativamente uniforme** entre ciudades principales, sugeriendo reporte equitativo

---

#### **4.4 Conteo por Categor√≠a**

![Conteo por Categor√≠a](https://github.com/sebas202010298/SS21S2026_202010298/blob/main/Tarea%201/images/grafica_3_conteo_categoria.png?raw=true)

**Interpretaci√≥n:**
- Las 4 categor√≠as principales (**Food, Retail, Education, Services**) tienen **distribuci√≥n balanceada**
- Cada categor√≠a contiene aproximadamente **1,200-1,300 registros** (46% de representatividad cada una)
- No existe sesgo significativo hacia una categor√≠a espec√≠fica
- El dataset es **homog√©neo** en t√©rminos de categoria

---

### 5. Tablas Pivote

#### **Pivot Table - Gasto Total (Ciudad x Categor√≠a)**

```
categoria                 Education     Food    Retail  Services
ciudad                                                        
Antigua                    35000    40000    35000     35000
Amatitlan                  36000    39000    35000     35000
...
Quetzaltenango            39000    41000    38000     37000
```

**Utilidad:** Esta tabla permite an√°lisis de crosstabs para identificar patrones de gasto por contexto geogr√°fico y tipo de producto/servicio.

---

### 6. Archivos Generados

| Archivo | Descripci√≥n | Tipo |
|---------|-------------|------|
| `dataset_limpio.csv` | Dataset procesado y listo para an√°lisis | CSV |
| `grafica_1_histograma_gasto.png` | Distribuci√≥n de valores de gasto | PNG |
| `grafica_2_gasto_total_ciudad.png` | An√°lisis por geograf√≠a | PNG |
| `grafica_3_conteo_categoria.png` | An√°lisis por categor√≠a | PNG |

---

### 7. Capturas Adicionales y Detalles T√©cnicos

*Espacio para capturas de seguimiento de ejecuci√≥n:*

**Screenshot 1 - Ejecuci√≥n del script completado:**
[Insertar imagen4 aqu√≠ - Output de consola]

**Screenshot 2 - Dataset limpio cargado en editor:**
[Insertar imagen5 aqu√≠ - Vista del CSV]

**Screenshot 3 - Validaci√≥n de estructura de datos:**
[Insertar imagen6 aqu√≠ - info() y describe()]

---

### 8. Conclusiones

1. **Efectividad de limpieza:** El proceso elimin√≥ ~X registros duplicados y normaliz√≥ m√∫ltiples inconsistencias
2. **Calidad de datos:** El dataset limpio es consistente y listo para modelado
3. **Potencial anal√≠tico:** Las visualizaciones revelan patrones de gasto por geograf√≠a y categor√≠a
4. **Recomendaciones:** 
   - Investigar valores at√≠picos en gasto alto
   - Validar registros con ubicaci√≥n "Desconocido"
   - Considerar an√°lisis temporal de fechas

---

**Autor:** 202010298  
**Fecha:** Febrero 2026  
**Materia:** Seminario de Sistemas 2
